{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvK3RlGhUVxtP9gwHoryoM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiSohanSajja/HaloScope-ReImplementation/blob/main/HaloScopeReImplementation_UWM_ProfSharonLi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeNAoOU-jkfP"
      },
      "outputs": [],
      "source": [
        "!pip -q install datasets transformers accelerate sentencepiece scikit-learn\n",
        "\n",
        "import os, re, pickle, string, math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/HaloScope_Data_v2\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "print(\"✅ SAVE_DIR:\", SAVE_DIR)\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_splits_from_tydiqa(test_n=200, val_n=100, unlabeled_n=800, seed=42):\n",
        "    \"\"\"\n",
        "    Build fixed-size splits (so your project is stable and fast).\n",
        "    We use TyDiQA secondary_task English subset.\n",
        "    \"\"\"\n",
        "    ds = load_dataset(\"google-research-datasets/tydiqa\", \"secondary_task\")\n",
        "\n",
        "    # collect English QAs\n",
        "    english = []\n",
        "    for split in [\"train\", \"validation\"]:\n",
        "        for item in ds[split]:\n",
        "            if str(item.get(\"id\",\"\")).startswith(\"english-\"):\n",
        "                answers = item.get(\"answers\", {})\n",
        "                if answers and \"text\" in answers and len(answers[\"text\"]) > 0:\n",
        "                    english.append({\n",
        "                        \"question\": item.get(\"question\",\"\"),\n",
        "                        \"context\": item.get(\"context\",\"\"),\n",
        "                        \"answer\": answers[\"text\"][0],\n",
        "                        \"id\": item[\"id\"]\n",
        "                    })\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    rng.shuffle(english)\n",
        "\n",
        "    # ensure enough data\n",
        "    total_needed = test_n + val_n + unlabeled_n\n",
        "    if len(english) < total_needed:\n",
        "        raise RuntimeError(f\"Not enough English samples. Needed {total_needed}, found {len(english)}\")\n",
        "\n",
        "    test = english[:test_n]\n",
        "    val = english[test_n:test_n+val_n]\n",
        "    unlabeled = english[test_n+val_n:test_n+val_n+unlabeled_n]\n",
        "\n",
        "    return {\"test\": test, \"val\": val, \"unlabeled\": unlabeled}\n",
        "\n",
        "splits_path = f\"{SAVE_DIR}/tydiqa_splits.pkl\"\n",
        "\n",
        "if os.path.exists(splits_path):\n",
        "    splits = pickle.load(open(splits_path, \"rb\"))\n",
        "    print(\"✅ Loaded existing splits:\", {k: len(v) for k,v in splits.items()})\n",
        "else:\n",
        "    splits = build_splits_from_tydiqa(test_n=200, val_n=100, unlabeled_n=800, seed=SEED)\n",
        "    pickle.dump(splits, open(splits_path, \"wb\"))\n",
        "    print(\"✅ Created & saved splits:\", {k: len(v) for k,v in splits.items()})\n"
      ],
      "metadata": {
        "id": "p5348sStjmU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "MODEL_NAME = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "model.eval()\n",
        "\n",
        "def generate_answer(question, context, max_ctx_chars=400, max_new_tokens=30):\n",
        "    prompt = (\n",
        "        \"Answer the question concisely using the context.\\n\"\n",
        "        f\"Context: {context[:max_ctx_chars]}\\n\"\n",
        "        f\"Q: {question}\\nA:\"\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=1)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "def generate_split(split_name, items, out_path):\n",
        "    if os.path.exists(out_path):\n",
        "        data = pickle.load(open(out_path, \"rb\"))\n",
        "        print(f\"✅ {split_name}: loaded existing {len(data)}\")\n",
        "        return data\n",
        "\n",
        "    generated = []\n",
        "    for it in tqdm(items, desc=f\"Generating {split_name}\"):\n",
        "        try:\n",
        "            gen = generate_answer(it[\"question\"], it[\"context\"])\n",
        "        except Exception:\n",
        "            gen = \"\"\n",
        "        generated.append({\n",
        "            \"question\": it[\"question\"],\n",
        "            \"context\": it[\"context\"],\n",
        "            \"ground_truth\": it[\"answer\"],   # ALWAYS present\n",
        "            \"generated_answer\": gen,        # ALWAYS present\n",
        "            \"id\": it[\"id\"]\n",
        "        })\n",
        "\n",
        "    pickle.dump(generated, open(out_path, \"wb\"))\n",
        "    print(f\"✅ {split_name}: saved {len(generated)} to {out_path}\")\n",
        "    return generated\n",
        "\n",
        "unlabeled_gen = generate_split(\"unlabeled\", splits[\"unlabeled\"], f\"{SAVE_DIR}/unlabeled_generated.pkl\")\n",
        "val_gen      = generate_split(\"val\",      splits[\"val\"],      f\"{SAVE_DIR}/val_generated.pkl\")\n",
        "test_gen     = generate_split(\"test\",     splits[\"test\"],     f\"{SAVE_DIR}/test_generated.pkl\")\n"
      ],
      "metadata": {
        "id": "V8hBsj9zkTEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_embedding(question, generated_answer, layer_idx=-2):\n",
        "    \"\"\"\n",
        "    Use encoder hidden states; last token embedding of selected layer.\n",
        "    \"\"\"\n",
        "    text = f\"Q: {question} A: {generated_answer}\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc = model.encoder(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "        hs = enc.hidden_states[layer_idx]      # [1, seq, d]\n",
        "        emb = hs[0, -1, :].cpu().numpy()       # last token\n",
        "    return emb\n",
        "\n",
        "def embed_split(generated_list, name):\n",
        "    embs = []\n",
        "    for r in tqdm(generated_list, desc=f\"Embedding {name}\"):\n",
        "        try:\n",
        "            embs.append(extract_embedding(r[\"question\"], r[\"generated_answer\"]))\n",
        "        except Exception:\n",
        "            embs.append(np.zeros(model.config.d_model, dtype=np.float32))\n",
        "    return np.array(embs, dtype=np.float32)\n",
        "\n",
        "emb_path = f\"{SAVE_DIR}/haloscope_embeddings.pkl\"\n",
        "if os.path.exists(emb_path):\n",
        "    halo = pickle.load(open(emb_path, \"rb\"))\n",
        "    print(\"✅ Loaded existing embeddings.\")\n",
        "else:\n",
        "    X_u = embed_split(unlabeled_gen, \"unlabeled\")\n",
        "    X_v = embed_split(val_gen, \"val\")\n",
        "    X_t = embed_split(test_gen, \"test\")\n",
        "\n",
        "    # HaloScope SVD on unlabeled\n",
        "    mean_u = X_u.mean(axis=0)\n",
        "    C = X_u - mean_u\n",
        "    U, S, Vt = np.linalg.svd(C, full_matrices=False)\n",
        "\n",
        "    def membership_score(X, mean, Vt, S, k=5):\n",
        "        centered = X - mean\n",
        "        scores = []\n",
        "        for i in range(centered.shape[0]):\n",
        "            sc = 0.0\n",
        "            for j in range(k):\n",
        "                proj = float(np.dot(centered[i], Vt[j]))\n",
        "                sc += float(S[j]) * (proj ** 2)\n",
        "            scores.append(sc / k)\n",
        "        return np.array(scores, dtype=np.float64)\n",
        "\n",
        "    k = 5\n",
        "    scores_u = membership_score(X_u, mean_u, Vt, S, k=k)\n",
        "\n",
        "    halo = {\n",
        "        \"X_unlabeled\": X_u,\n",
        "        \"X_val\": X_v,\n",
        "        \"X_test\": X_t,\n",
        "        \"mean_u\": mean_u,\n",
        "        \"U\": U,\n",
        "        \"S\": S,\n",
        "        \"Vt\": Vt,\n",
        "        \"k\": k,\n",
        "        \"scores_unlabeled\": scores_u\n",
        "    }\n",
        "    pickle.dump(halo, open(emb_path, \"wb\"))\n",
        "    print(\"✅ Saved embeddings + HaloScope to:\", emb_path)\n",
        "\n",
        "print(\"Shapes:\",\n",
        "      halo[\"X_unlabeled\"].shape,\n",
        "      halo[\"X_val\"].shape,\n",
        "      halo[\"X_test\"].shape)\n",
        "print(\"Score stats (unlabeled): min/mean/max\",\n",
        "      float(halo[\"scores_unlabeled\"].min()),\n",
        "      float(halo[\"scores_unlabeled\"].mean()),\n",
        "      float(halo[\"scores_unlabeled\"].max()))\n"
      ],
      "metadata": {
        "id": "ZKAhEIIEkb8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_u = halo[\"X_unlabeled\"]\n",
        "scores_u = halo[\"scores_unlabeled\"]\n",
        "\n",
        "# pseudo labels: truthful=1 for low score, hallucinated=0 for high score\n",
        "threshold = np.percentile(scores_u, 70)\n",
        "y_pseudo = (scores_u <= threshold).astype(np.float32)  # 1 truthful, 0 hallucinated\n",
        "\n",
        "print(\"Pseudo-label split:\",\n",
        "      \"truthful\", int(y_pseudo.sum()),\n",
        "      \"hallucinated\", int(len(y_pseudo)-y_pseudo.sum()))\n",
        "\n",
        "class TruthfulnessClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=512, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "clf = TruthfulnessClassifier(input_dim=X_u.shape[1], hidden_dim=1024)\n",
        "opt = optim.SGD(clf.parameters(), lr=0.05, weight_decay=3e-4)\n",
        "crit = nn.BCEWithLogitsLoss()\n",
        "\n",
        "X_train = torch.tensor(X_u, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_pseudo.reshape(-1,1), dtype=torch.float32)\n",
        "\n",
        "epochs = 50\n",
        "losses = []\n",
        "clf.train()\n",
        "for ep in range(epochs):\n",
        "    opt.zero_grad()\n",
        "    logits = clf(X_train)\n",
        "    loss = crit(logits, y_train)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    losses.append(float(loss.item()))\n",
        "    if (ep+1) % 10 == 0:\n",
        "        print(f\"Epoch {ep+1:02d}/{epochs}  loss={loss.item():.4f}\")\n",
        "\n",
        "# Save SAFE checkpoint (state_dict only) -> avoids future torch.load issues\n",
        "ckpt = {\n",
        "    \"state_dict\": clf.state_dict(),\n",
        "    \"input_dim\": X_u.shape[1],\n",
        "    \"hidden_dim\": 1024,\n",
        "    \"threshold\": float(threshold),\n",
        "    \"losses\": losses\n",
        "}\n",
        "torch.save(ckpt, f\"{SAVE_DIR}/classifier_state.pt\")\n",
        "print(\"✅ Saved classifier to:\", f\"{SAVE_DIR}/classifier_state.pt\")\n",
        "\n",
        "# Save loss curve\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training Loss (Pseudo-labels)\")\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"{SAVE_DIR}/training_loss.png\", dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(\"✅ Saved loss plot:\", f\"{SAVE_DIR}/training_loss.png\")\n"
      ],
      "metadata": {
        "id": "MpZ7WdvxmHcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Labeling utilities\n",
        "# -----------------------------\n",
        "def normalize_text(s):\n",
        "    s = s.lower().strip()\n",
        "    s = s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def exact_match_label(records):\n",
        "    # truthful=0, hallucinated=1 (your convention)\n",
        "    y = []\n",
        "    for r in records:\n",
        "        gt = normalize_text(r[\"ground_truth\"])\n",
        "        gen = normalize_text(r[\"generated_answer\"])\n",
        "        y.append(0 if gen == gt else 1)\n",
        "    return np.array(y, dtype=int)\n",
        "\n",
        "def soft_match_label(records):\n",
        "    \"\"\"\n",
        "    Less harsh than exact match:\n",
        "    truthful (0) if:\n",
        "      - gt is substring of gen OR gen is substring of gt OR token overlap >= 0.5\n",
        "    else hallucinated (1)\n",
        "    \"\"\"\n",
        "    y = []\n",
        "    for r in records:\n",
        "        gt = normalize_text(r[\"ground_truth\"])\n",
        "        gen = normalize_text(r[\"generated_answer\"])\n",
        "        if not gt or not gen:\n",
        "            y.append(1); continue\n",
        "\n",
        "        if gt in gen or gen in gt:\n",
        "            y.append(0); continue\n",
        "\n",
        "        gt_set = set(gt.split())\n",
        "        gen_set = set(gen.split())\n",
        "        if len(gt_set) == 0:\n",
        "            y.append(1); continue\n",
        "        overlap = len(gt_set & gen_set) / len(gt_set)\n",
        "        y.append(0 if overlap >= 0.5 else 1)\n",
        "    return np.array(y, dtype=int)\n",
        "\n",
        "# Create labels for val/test\n",
        "y_val_em  = exact_match_label(val_gen)\n",
        "y_test_em = exact_match_label(test_gen)\n",
        "y_val_sm  = soft_match_label(val_gen)\n",
        "y_test_sm = soft_match_label(test_gen)\n",
        "\n",
        "print(\"Label balance (VAL)  EM truthful/hallu:\", int((y_val_em==0).sum()), int((y_val_em==1).sum()))\n",
        "print(\"Label balance (TEST) EM truthful/hallu:\", int((y_test_em==0).sum()), int((y_test_em==1).sum()))\n",
        "print(\"Label balance (VAL)  SM truthful/hallu:\", int((y_val_sm==0).sum()), int((y_val_sm==1).sum()))\n",
        "print(\"Label balance (TEST) SM truthful/hallu:\", int((y_test_sm==0).sum()), int((y_test_sm==1).sum()))\n",
        "\n",
        "# -----------------------------\n",
        "# Load classifier safely\n",
        "# -----------------------------\n",
        "ckpt = torch.load(f\"{SAVE_DIR}/classifier_state.pt\", map_location=\"cpu\")\n",
        "clf2 = TruthfulnessClassifier(ckpt[\"input_dim\"], ckpt[\"hidden_dim\"])\n",
        "clf2.load_state_dict(ckpt[\"state_dict\"])\n",
        "clf2.eval()\n",
        "\n",
        "X_val = torch.tensor(halo[\"X_val\"], dtype=torch.float32)\n",
        "X_test = torch.tensor(halo[\"X_test\"], dtype=torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # model outputs probability of pseudo \"truthful\" (because trained with y_pseudo: 1 truthful)\n",
        "    p_val_truth = torch.sigmoid(clf2(X_val)).squeeze().cpu().numpy()\n",
        "    p_test_truth = torch.sigmoid(clf2(X_test)).squeeze().cpu().numpy()\n",
        "\n",
        "# We need probability of \"hallucinated\" for AUROC with label=1 hallucinated\n",
        "p_val_hallu  = 1.0 - p_val_truth\n",
        "p_test_hallu = 1.0 - p_test_truth\n",
        "\n",
        "def eval_split(name, y_true, p_hallu):\n",
        "    pred = (p_hallu >= 0.5).astype(int)  # 1 hallucinated\n",
        "    acc = accuracy_score(y_true, pred)\n",
        "    auc = roc_auc_score(y_true, p_hallu) if len(np.unique(y_true)) == 2 else float(\"nan\")\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"AUROC:    {auc:.4f}\")\n",
        "    print(\"Confusion:\\n\", confusion_matrix(y_true, pred))\n",
        "    return acc, auc\n",
        "\n",
        "print(\"\\n==================== FINAL RESULTS ====================\")\n",
        "print(\"Note: label=1 means 'hallucinated', label=0 means 'truthful'\")\n",
        "print(\"=======================================================\")\n",
        "\n",
        "# Exact match (strict)\n",
        "eval_split(\"VAL  (Exact Match labels)\",  y_val_em,  p_val_hallu)\n",
        "eval_split(\"TEST (Exact Match labels)\",  y_test_em, p_test_hallu)\n",
        "\n",
        "# Soft match (recommended)\n",
        "eval_split(\"VAL  (Soft Match labels)\",   y_val_sm,  p_val_hallu)\n",
        "eval_split(\"TEST (Soft Match labels)\",   y_test_sm, p_test_hallu)\n",
        "\n",
        "print(\"\\n✅ Done. Metrics above are your final visible output.\")\n"
      ],
      "metadata": {
        "id": "CRmYnS4hmZbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pseudo truthful %:\", y_pseudo.mean()*100)\n"
      ],
      "metadata": {
        "id": "DOy0z3L6medw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"p_test_truth stats:\",\n",
        "      \"min\", float(p_test_truth.min()),\n",
        "      \"max\", float(p_test_truth.max()),\n",
        "      \"mean\", float(p_test_truth.mean()),\n",
        "      \"std\", float(p_test_truth.std()))\n",
        "\n",
        "print(\"p_test_hallu stats:\",\n",
        "      \"min\", float(p_test_hallu.min()),\n",
        "      \"max\", float(p_test_hallu.max()),\n",
        "      \"mean\", float(p_test_hallu.mean()),\n",
        "      \"std\", float(p_test_hallu.std()))\n"
      ],
      "metadata": {
        "id": "spJ9E-NloFHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pickle, numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/HaloScope_Data_v2\"\n",
        "\n",
        "# Load HaloScope stuff\n",
        "halo = pickle.load(open(f\"{SAVE_DIR}/haloscope_embeddings.pkl\", \"rb\"))\n",
        "X_u = halo[\"X_unlabeled\"].astype(np.float32)\n",
        "scores_u = halo[\"scores_unlabeled\"]\n",
        "\n",
        "# Pseudo labels: 1 truthful (low score), 0 hallucinated (high score)\n",
        "threshold = np.percentile(scores_u, 70)\n",
        "y_pseudo = (scores_u <= threshold).astype(np.float32)\n",
        "\n",
        "print(\"Pseudo-label balance:\")\n",
        "print(\"  truthful (1):\", int(y_pseudo.sum()))\n",
        "print(\"  halluc  (0):\", int(len(y_pseudo) - y_pseudo.sum()))\n",
        "\n",
        "# Model\n",
        "class TruthfulnessClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=512, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "clf = TruthfulnessClassifier(input_dim=X_u.shape[1], hidden_dim=1024)\n",
        "\n",
        "# ✅ Class-weighted BCE to avoid majority-class collapse\n",
        "pos = float(y_pseudo.sum())\n",
        "neg = float(len(y_pseudo) - y_pseudo.sum())\n",
        "pos_weight = torch.tensor([neg / (pos + 1e-8)], dtype=torch.float32)\n",
        "crit = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "print(\"Using pos_weight:\", pos_weight.item())\n",
        "\n",
        "opt = optim.AdamW(clf.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# Mini-batches\n",
        "X_train = torch.tensor(X_u, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_pseudo.reshape(-1,1), dtype=torch.float32)\n",
        "loader = DataLoader(TensorDataset(X_train, y_train), batch_size=128, shuffle=True)\n",
        "\n",
        "# Train\n",
        "clf.train()\n",
        "for ep in range(10):  # 10 epochs is enough to see signal; you can increase to 30\n",
        "    total = 0.0\n",
        "    for xb, yb in loader:\n",
        "        opt.zero_grad()\n",
        "        logits = clf(xb)\n",
        "        loss = crit(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total += float(loss.item())\n",
        "    print(f\"Epoch {ep+1:02d} | loss={total/len(loader):.4f}\")\n",
        "\n",
        "# Save safely\n",
        "ckpt = {\"state_dict\": clf.state_dict(), \"input_dim\": X_u.shape[1], \"hidden_dim\": 1024}\n",
        "torch.save(ckpt, f\"{SAVE_DIR}/classifier_state_fixed.pt\")\n",
        "print(\"✅ Saved:\", f\"{SAVE_DIR}/classifier_state_fixed.pt\")\n",
        "\n",
        "# ---- Quick check: are outputs still constant? ----\n",
        "clf.eval()\n",
        "X_test = torch.tensor(halo[\"X_test\"].astype(np.float32))\n",
        "with torch.no_grad():\n",
        "    p_truth = torch.sigmoid(clf(X_test)).squeeze().cpu().numpy()\n",
        "\n",
        "print(\"\\nOutput stats on TEST:\")\n",
        "print(\"p_truth min/max/mean/std:\", float(p_truth.min()), float(p_truth.max()), float(p_truth.mean()), float(p_truth.std()))\n"
      ],
      "metadata": {
        "id": "SmqIokLLp593"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pickle, numpy as np\n",
        "import torch, torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
        "import re, string\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/HaloScope_Data_v2\"\n",
        "\n",
        "# ---- Load trained classifier ----\n",
        "ckpt = torch.load(f\"{SAVE_DIR}/classifier_state_fixed.pt\", map_location=\"cpu\")\n",
        "\n",
        "class TruthfulnessClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=512, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = TruthfulnessClassifier(ckpt[\"input_dim\"], ckpt[\"hidden_dim\"])\n",
        "model.load_state_dict(ckpt[\"state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "# ---- Load embeddings + generated data ----\n",
        "halo = pickle.load(open(f\"{SAVE_DIR}/haloscope_embeddings.pkl\", \"rb\"))\n",
        "X_test = halo[\"X_test\"].astype(np.float32)\n",
        "X_val  = halo[\"X_val\"].astype(np.float32)\n",
        "\n",
        "test_gen = pickle.load(open(f\"{SAVE_DIR}/test_generated.pkl\", \"rb\"))\n",
        "val_gen  = pickle.load(open(f\"{SAVE_DIR}/val_generated.pkl\", \"rb\"))\n",
        "\n",
        "# ---- Soft labels (recommended) ----\n",
        "def normalize_text(s):\n",
        "    s = s.lower().strip()\n",
        "    s = s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def soft_match_label(records):\n",
        "    y = []\n",
        "    for r in records:\n",
        "        gt = normalize_text(r[\"ground_truth\"])\n",
        "        gen = normalize_text(r[\"generated_answer\"])\n",
        "        if not gt or not gen:\n",
        "            y.append(1); continue\n",
        "        if gt in gen or gen in gt:\n",
        "            y.append(0); continue\n",
        "        gt_set = set(gt.split())\n",
        "        gen_set = set(gen.split())\n",
        "        overlap = len(gt_set & gen_set) / max(len(gt_set), 1)\n",
        "        y.append(0 if overlap >= 0.5 else 1)  # 1 = hallucinated\n",
        "    return np.array(y, dtype=int)\n",
        "\n",
        "y_test = soft_match_label(test_gen)\n",
        "y_val  = soft_match_label(val_gen)\n",
        "\n",
        "# ---- Predict hallucination probability ----\n",
        "with torch.no_grad():\n",
        "    p_truth_test = torch.sigmoid(model(torch.tensor(X_test))).squeeze().numpy()\n",
        "    p_truth_val  = torch.sigmoid(model(torch.tensor(X_val ))).squeeze().numpy()\n",
        "\n",
        "p_hallu_test = 1.0 - p_truth_test\n",
        "p_hallu_val  = 1.0 - p_truth_val\n",
        "\n",
        "pred_test = (p_hallu_test >= 0.5).astype(int)\n",
        "pred_val  = (p_hallu_val  >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\n================ FINAL OUTPUT (SOFT LABELS) ================\")\n",
        "print(f\"VAL  Accuracy: {accuracy_score(y_val, pred_val):.4f}\")\n",
        "print(f\"VAL  AUROC:    {roc_auc_score(y_val, p_hallu_val):.4f}\")\n",
        "print(\"VAL Confusion:\\n\", confusion_matrix(y_val, pred_val))\n",
        "print(\"-----------------------------------------------------------\")\n",
        "print(f\"TEST Accuracy: {accuracy_score(y_test, pred_test):.4f}\")\n",
        "print(f\"TEST AUROC:    {roc_auc_score(y_test, p_hallu_test):.4f}\")\n",
        "print(\"TEST Confusion:\\n\", confusion_matrix(y_test, pred_test))\n",
        "print(\"\\nTEST report:\\n\", classification_report(y_test, pred_test, zero_division=0))\n",
        "print(\"===========================================================\\n\")\n"
      ],
      "metadata": {
        "id": "1Vkh_3Ygrf3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "SRC = \"/content/drive/MyDrive/HaloScope_Data_v2\"\n",
        "DST = \"/content/HaloScope-Reimplementation\"\n",
        "\n",
        "# fresh folder\n",
        "if os.path.exists(DST):\n",
        "    shutil.rmtree(DST)\n",
        "os.makedirs(DST, exist_ok=True)\n",
        "\n",
        "# make subfolders\n",
        "os.makedirs(f\"{DST}/results\", exist_ok=True)\n",
        "os.makedirs(f\"{DST}/models\", exist_ok=True)\n",
        "\n",
        "# copy key files (adjust names if needed)\n",
        "for f in [\"training_loss.png\", \"training_curve.png\"]:\n",
        "    p = os.path.join(SRC, f)\n",
        "    if os.path.exists(p):\n",
        "        shutil.copy(p, f\"{DST}/results/{f}\")\n",
        "\n",
        "# model (optional)\n",
        "model_path = os.path.join(SRC, \"classifier_state_fixed.pt\")\n",
        "if os.path.exists(model_path):\n",
        "    shutil.copy(model_path, f\"{DST}/models/classifier_state_fixed.pt\")\n",
        "\n",
        "print(\"✅ Prepared folder:\", DST)\n",
        "print(\"Files:\", os.listdir(DST), \"results:\", os.listdir(f\"{DST}/results\"), \"models:\", os.listdir(f\"{DST}/models\"))\n"
      ],
      "metadata": {
        "id": "GmwZ-7C2sjro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "readme = \"\"\"# HaloScope-style Hallucination Detection (Reimplementation)\n",
        "\n",
        "End-to-end reimplementation of a HaloScope-inspired pipeline for detecting hallucinated LLM answers\n",
        "using embedding geometry + weak supervision.\n",
        "\n",
        "## Pipeline\n",
        "1. Download TyDiQA (English)\n",
        "2. Generate answers using FLAN-T5-small\n",
        "3. Extract encoder embeddings\n",
        "4. SVD subspace membership scoring (HaloScope core idea)\n",
        "5. Pseudo-label unlabeled set from membership scores\n",
        "6. Train MLP classifier\n",
        "7. Evaluate with soft-match correctness labels\n",
        "\n",
        "## Results (Soft-match labels)\n",
        "Validation: Accuracy = 0.61, AUROC = 0.6627\n",
        "Test: Accuracy = 0.58, AUROC = 0.5778\n",
        "\n",
        "## Notes\n",
        "- Labels are heuristic (soft-match), not human annotations.\n",
        "- Best interpreted as a ranking signal (AUROC), not a perfect classifier.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/HaloScope-Reimplementation/README.md\", \"w\") as f:\n",
        "    f.write(readme)\n",
        "\n",
        "results = \"\"\"FINAL OUTPUT (SOFT LABELS)\n",
        "VAL  Accuracy: 0.6100\n",
        "VAL  AUROC:    0.6627\n",
        "VAL Confusion:\n",
        "[[34 32]\n",
        " [ 7 27]]\n",
        "\n",
        "TEST Accuracy: 0.5800\n",
        "TEST AUROC:    0.5778\n",
        "TEST Confusion:\n",
        "[[84 49]\n",
        " [35 32]]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/HaloScope-Reimplementation/results/final_results.txt\", \"w\") as f:\n",
        "    f.write(results)\n",
        "\n",
        "print(\"✅ README + results file created\")\n"
      ],
      "metadata": {
        "id": "OddceR2CuXEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "DST = \"/content/HaloScope-Reimplementation\"\n",
        "\n",
        "print(\"DST exists?\", os.path.exists(DST))\n",
        "print(\"DST files:\", os.listdir(DST))\n",
        "print(\"results files:\", os.listdir(f\"{DST}/results\"))\n",
        "print(\"models files:\", os.listdir(f\"{DST}/models\"))\n"
      ],
      "metadata": {
        "id": "aG2SBbFPvWPy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}